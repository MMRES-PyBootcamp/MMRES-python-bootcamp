{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MMRES-PyBootcamp/MMRES-python-bootcamp2024/blob/master/06_Plotting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12 - TODO (50')\n",
    "> TODO An introduction to *data visualization* with Seaborn in a purely hands-on approach. Here we will learn how to get beautiful plots using \"real life data\". In particular, we will try to reproduce some plots appearing in a [Nature paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7960507/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    " * [Loading a dataset](#Loading-a-dataset)\n",
    " * [Variance and covariance](#Variance-and-covariance)\n",
    " * [The covariance matrix](#The-covariance-matrix)\n",
    " * [The essence behind of the covariance matrix](#The-essence-behind-of-the-covariance-matrix)\n",
    "   * [Standardizing the covariance matrix](#Standardizing-the-covariance-matrix)\n",
    "   * [Eigenthings of the covariance matrix](#Eigenthings-of-the-covariance-matrix)\n",
    "   * [The meaning of the covariance matrix](#The-meaning-of-the-covariance-matrix)\n",
    " * [Principal component analysis](#Principal-component-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is devised as a tool to enable your **self-learning process**. If you get stuck at some step or need any kind of help, please don't hesitate to raise your hand and ask for the teacher's guidance. Along it, you will find some **special cells**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b> Practice cells announce exercises that you should try during the current boot camp session. Usually, solutions are provided using hidden cells (look for the dot dot dot symbol \"...\" and click to unravel them and check that your try is correct).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b> Extension cells correspond to exercises (or links to contents) that are a bit more advanced. We recommend to try them after the current boot camp session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Tip cells just give some advice or complementary information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caveat:</b> Caveat cells warn you about the most common pitfalls one founds when starts his/her path learning Python.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages with their corresponding alias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset\n",
    "\n",
    "Let's stats by loading again the IQ dataset [online resource](https://lectures.scientific-python.org/packages/statistics/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FWO0aVaNX09J",
    "outputId": "518b36bc-014c-45e5-a0f8-58d66109069d"
   },
   "outputs": [],
   "source": [
    "# Loading the \"Brain Size\" dataset directly from an URL\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://scipy-lectures.org/_downloads/brain_size.csv',\n",
    "    sep=';',\n",
    "    na_values=\".\",\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "# Show df's head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that the columns *FSIQ*, *VIQ* and *PIQ* stand for different \"intelligence quotients\" and the *MRI_Count* is another \"intelligence\" measure based on magnetic resonance imaging. The columns *Gender*, *Weight* and *Height* express the gender, the weight (in pounds) and the height (in inches) of the individuals, respectively. As usual, let's express *Weight* and *Height* column values to metric units before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from imperial units to metric units\n",
    "df['Weight [kg]'] = df['Weight'] * 0.45359237\n",
    "df['Height [m]'] = df['Height'] * 0.0254\n",
    "\n",
    "# Drop imperial columns\n",
    "df.drop(columns=['Weight', 'Height'], inplace=True)\n",
    "\n",
    "# Show df's head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and covariance\n",
    "\n",
    "Suppose that we have random variable $x = \\left( x_i, \\dots, x_n \\right)$, then, its *variance* is defined as the *expected value of the squared deviation between $x$ and its expected value $E[x]$*:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_x = E \\left[ \\left( x - E \\left[ x \\right] \\right)^2  \\right],\n",
    "\\end{equation}\n",
    "\n",
    "which in the particular case of a discrete equally likely random variable becomes: \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_x = \\frac{1}{N} \\sum_{i=1}^n{\\left( x_i - \\mu_x \\right)^2}\n",
    "\\end{equation}.\n",
    "\n",
    "Let's try to implement this last $\\sigma^2_{x}$ expression using *NumPy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a \"seeded\" random number generator\n",
    "rnd_seed = np.random.default_rng(seed=1985)\n",
    "\n",
    "# Generate a random x variable following a uniform distribution\n",
    "x = rnd_seed.uniform(low=-1, high=+1, size=15)\n",
    "\n",
    "# Compute the variance of x by hand\n",
    "sigma2_x = (1 / len(x)) * sum( (x - x.mean())**2 )\n",
    "\n",
    "# Show the result\n",
    "print(sigma2_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's quickly check if our variance implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance of x with NumPy and show it\n",
    "np.var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, now let's move on! The *covariance* of two random variables $x = \\left( x_i, \\dots, x_n \\right)$ and $y = \\left( y_i, \\dots, y_n \\right)$ is defined as the *expected value of the product of the deviations between those variables ($x$ and $y$) and their corresponding expected values ($E[x]$ and $E[y]$)*:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{xy} = E \\left[ \\sigma^2_x \\sigma^2_y \\right] = E \\left[ \\left( x - E \\left[ x \\right] \\right) \\left( y - E \\left[ y \\right] \\right) \\right],\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "which in the particular case of a discrete equally likely random variable becomes: \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{xy} = \\frac{1}{N} \\sum_{i=1}^n{\\left( x_i - \\mu_x \\right) \\left( y_i - \\mu_y \\right)}\n",
    "\\end{equation}.\n",
    "\n",
    "As we just did for $\\sigma^2_{x}$, let's try to implement the $\\sigma^2_{xy}$ expression using **NumPy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a \"seeded\" random number generator\n",
    "rnd_seed = np.random.default_rng(seed=1985)\n",
    "\n",
    "# Generate random x and y variables following a uniform distribution\n",
    "x = rnd_seed.uniform(low=-1, high=+1, size=15)\n",
    "y = rnd_seed.uniform(low=-1, high=+1, size=15)\n",
    "\n",
    "# Compute the covariance between x and y by hand\n",
    "sigma2_xy = (1 / len(x)) * sum( (x - x.mean()) * (y - y.mean()) )\n",
    "print(sigma2_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, let's quickly check our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance between x and y with NumPy and show it\n",
    "np.cov(x, y, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The covariance matrix\n",
    "\n",
    "Note that when we asked **NumPy** to compute the covariance between $x$ and $y$, we got a $2 \\times 2$ symmetric matrix. The off-diagonal elements in this matrix are the covariance between $x$ and $y$ ($\\sigma^2_{xy}$) and the covariance between $y$ and $x$ ($\\sigma^2_{yx}$), which, as expected, gave identical estimates. Analogously, the diagonal elements in this matrix are the covariance of $x$ with itself ($\\sigma^2_{xx}$, AKA the variance of $x$, $\\sigma^2_{x}$) and the covariance of $y$ with itself ($\\sigma^2_{yy}$, AKA the variance of $y$, $\\sigma^2_{y}$):\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{pmatrix}\n",
    " 0.3125 & -0.0888 \\\\\n",
    "-0.0888 & 0.2506\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    " \\sigma^2_{xx} & \\sigma^2_{xy} \\\\\n",
    "\\sigma^2_{yx} & \\sigma^2_{yy}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    " \\sigma^2_{x} & \\sigma^2_{xy} \\\\\n",
    "\\sigma^2_{yx} & \\sigma^2_{y}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance between x and y by hand (sigma2_xx)\n",
    "sigma2_xx = (1 / len(x)) * sum( (x - x.mean()) * (x - x.mean()) )\n",
    "\n",
    "# Compute the covariance between x and y by hand (sigma2_xy)\n",
    "sigma2_xy = (1 / len(x)) * sum( (x - x.mean()) * (y - y.mean()) )\n",
    "\n",
    "# Compute the covariance between x and y by hand (sigma2_yx)\n",
    "sigma2_yx = (1 / len(y)) * sum( (y - y.mean()) * (x - x.mean()) )\n",
    "\n",
    "# Compute the covariance between x and y by hand (sigma2_yy)\n",
    "sigma2_yy = (1 / len(y)) * sum( (y - y.mean()) * (y - y.mean()) )\n",
    "\n",
    "# Arrange the sigma2_xx, sigma2_xy, sigma2_yx and sigma2_yy as a 2x2 matrix\n",
    "np.array([ [sigma2_xx, sigma2_xy], [sigma2_yx, sigma2_yy] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the *covariance matrix* of the variables $x$ and $y$. Try to imagine how it would be, for example, the covariance matrix between three variables $x$, $y$ and $z$ (an so on for and arbitrary number of variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<b>Practice 1:</b> Get the **covariance matrix** between the variables **height** and **weight** in the IQ dataset.\n",
    "\n",
    "1) In the 1<sup>st</sup> code cell below, use the <b>NumPy</b> function `np.cov()` to compute the *covariance matrix* between the variables `Height [m]` and `Weight [kg]` in `df`. What happened in this first try?\n",
    "2) In the 2<sup>nd</sup> code cell below, use the <b>NumPy</b> function `np.cov()` to compute the *covariance matrix* between the variables `Height [m]` and `Weight [kg]` in `df_DropNan`. What happened in this second try?\n",
    "\n",
    "<b>Note:</b> Uncomment and fill only those code lines with underscores `___`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1st code cell)\n",
    "\n",
    "# Compute the covariance between Height [m] and Weight [kg] with NumPy and show it\n",
    "#print(___(___, ___, bias=True))\n",
    "\n",
    "# Show df's head\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (1st code cell)\n",
    "\n",
    "# Compute the covariance between Height [m] and Weight [kg] with NumPy and show it\n",
    "print(np.cov(df['Height [m]'], df['Weight [kg]'], bias=True))\n",
    "\n",
    "# Show df's head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2nd code cell)\n",
    "\n",
    "# Dropping all rows with NANs to enable covariance computation\n",
    "df_DropNan = df.dropna().copy()\n",
    "\n",
    "# Compute the covariance between Height [m] and Weight [kg] with NumPy and show it\n",
    "#print(___(___, ___, bias=True))\n",
    "\n",
    "# Show df_DropNan's head\n",
    "#df_DropNan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (2nd code cell)\n",
    "\n",
    "# Dropping all rows with NANs to enable covariance computation\n",
    "df_DropNan = df.dropna().copy()\n",
    "\n",
    "# Compute the covariance between Height [m] and Weight [kg] with NumPy and show it\n",
    "print(np.cov(df_DropNan['Height [m]'], df_DropNan['Weight [kg]'], bias=True))\n",
    "\n",
    "# Show df_DropNan's head\n",
    "df_DropNan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 1 ends here.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essence behind of the covariance matrix\n",
    "\n",
    "### Standardizing the covariance matrix\n",
    "\n",
    "Now, we will use the particular example of the covariance between `Height [m]` and `Weight [kg]` to get an intuitive understanding of this covariance matrix. As usual, let's start by visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot with Plotnine (metric data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Height [m]', y='Weight [kg]'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify a bit some of the steps downstream, let's standardize this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the z-score for 'Height [m]' and 'Weight [kg]'\n",
    "df_DropNan['Z-Height'] = (df_DropNan['Height [m]'] - np.mean(df_DropNan['Height [m]'])) / np.std(df_DropNan['Height [m]'])\n",
    "df_DropNan['Z-Weight'] = (df_DropNan['Weight [kg]'] - np.mean(df_DropNan['Weight [kg]'])) / np.std(df_DropNan['Weight [kg]'])\n",
    "\n",
    "# Show df_DropNan's head\n",
    "df_DropNan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Z-Height', y='Z-Weight'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both scatter plots are identical, with the unique difference being the scale of both axes. Now, let's compute the covariance matrix for the z-score data we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance between 'Z-Height' and 'Z-Weight' with NumPy and show it\n",
    "Sigma_Z = np.cov(df_DropNan['Z-Height'], df_DropNan['Z-Weight'], bias=True)\n",
    "\n",
    "# Show the covariance matrix\n",
    "Sigma_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the *covariance matrix* of the z-score data has *ones* in the diagonal in contrast to its non z-scored counterpart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance between 'Height' and 'Weight' with NumPy and show it\n",
    "Sigma = np.cov(df_DropNan['Height [m]'], df_DropNan['Weight [kg]'], bias=True)\n",
    "\n",
    "# Show the covariance matrix\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something natural because, when we z-scored the data, we manually bring the distribution of values to a standardized distribution with variance 1 and mean 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenthings of the covariance matrix\n",
    "\n",
    "Now, if we look at the covariance matrix through the prism of *Linear Algebra*, we could understand this $2 \\times 2$ matrix as a *linear transformation in a two-dimensional space*... Remember that *linear transformation* ($\\textbf{M}$) is just something that \"transmutes\" an input vector ($\\vec{v}$) to an output vector ($\\vec{u}$). Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a \"seeded\" random number generator\n",
    "rnd_seed = np.random.default_rng(seed=1985)\n",
    "\n",
    "# Create a random vector \"v\" (2 x 1)\n",
    "v = rnd_seed.integers(low=-5, high=5, size=2)\n",
    "\n",
    "# Create a random lineal transformation \"M\" (2 x 2)\n",
    "M = rnd_seed.integers(low=-5, high=5, size=(2, 2))\n",
    "\n",
    "# Apply \"M\" to \"v\" and store the output vector \"u\" (2 x 1)\n",
    "u = M @ v\n",
    "\n",
    "# Print \"v\", \"M\" and \"u\"\n",
    "print(f'v = {v}', f'M = {M}', f'u = {u}', sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\textbf{M} \\vec{v} = \\begin{pmatrix}  -4 & -1 \\\\ -1 & 0 \\end{pmatrix} \\cdot \\begin{pmatrix} -2 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} (-4) \\cdot (-2) + (-1) \\cdot (-3) \\\\ (-1) \\cdot (-2) + 0 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 8 + 3 \\\\ 2 + 0 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 2  \\end{pmatrix} = \\vec{u}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"transmutations\" cannot be arbitrary as they have to preserve certain vector operations (vector additivity and vector scalar multiplication). Intuitively, after passing through a linear transformation, the original direction of $\\vec{v}$ can be (but no necessarily) *rotated* and the original length of $\\vec{v}$ can be (but no necessarily) *elongated* or *shortened*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input \"v\" (red) and output \"u\" (green) vectors\n",
    "gg = (\n",
    "    p9.ggplot(data=df)\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=v[0], yend=v[1],\n",
    "        color='red',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=u[0], yend=u[1],\n",
    "        color='green',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    "    + p9.xlim(-3, 11) + p9.ylim(-3, 11) + p9.xlab('x') + p9.ylab('y')\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear transformations present certain special directions (two in the particular case of two-dimensional linear transformations). All the vectors living in these special directions will keep their directions unchanged after passing through the linear transformation. In other words, these vectors can be *elongated* or *shortened* but never *rotated* when passing through the linear transformation. This is precisely the definition of *eigenvector*: a vector $\\vec{v}$ that is proportional to itself, $\\lambda \\vec{v}$, after passing through the linear transformation $\\textbf{M}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{M} \\vec{v} = \\vec{u} = \\lambda \\vec{v},\n",
    "\\end{equation}\n",
    "\n",
    "where this proportional constant $\\lambda$ is the *eigenvalue* associated to the eigenvector $\\vec{v}$. Let's get the eigenvectors and eigenvalues of $\\textbf{M}$ leveraging the **NumPy** function [`np.linalg.eig()`](https://numpy.org/doc/2.3/reference/generated/numpy.linalg.eig.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eigenvectors and eigenvalues of M\n",
    "eigenthings = np.linalg.eig(M)\n",
    "\n",
    "# Print the eigenvectors and eigenvalues of M\n",
    "print(eigenthings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Caveat:</b> Keep in mind that `np.linalg.eig()` return the eigenvectors in a matrix <b><u>as columns</u></b>, (not as rows!):\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{pmatrix} \\vec{v}_1 & \\vec{v}_2 \\end{pmatrix} =\n",
    " \\begin{pmatrix} \\begin{pmatrix} v_{11} \\\\ v_{21} \\end{pmatrix} & \\begin{pmatrix} v_{12} \\\\ v_{22} \\end{pmatrix} \\end{pmatrix} =\n",
    "  \\begin{pmatrix} \\begin{pmatrix} -0.9732 \\\\ -0.2298 \\end{pmatrix} & \\begin{pmatrix} 0.2298 \\\\ -0.9732 \\end{pmatrix} \\end{pmatrix} =\n",
    " \\begin{pmatrix} -0.9732 & 0.2298 \\\\ -0.2298 & -0.9732 \\end{pmatrix}\n",
    " \\Rightarrow  \\begin{cases}\n",
    "      \\vec{v}_1 = \\begin{pmatrix} -0.9732 \\\\ -0.2298 \\end{pmatrix} \\\\\n",
    "      \\\\\n",
    "      \\vec{v}_2 = \\begin{pmatrix} 0.2298 \\\\ -0.9732 \\end{pmatrix} \\\\\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the first (0) and the second (1) eigenvectors\n",
    "e0 = eigenthings.eigenvectors[:, 0]\n",
    "e1 = eigenthings.eigenvectors[:, 1]\n",
    "print('Eigenvectors:', e0, e1, '', sep='\\n')\n",
    "\n",
    "# Store the first (0) and the second (1) eigenvalues\n",
    "λ0 = eigenthings.eigenvalues[0]\n",
    "λ1 = eigenthings.eigenvalues[1]\n",
    "print('Eigenvalues:', λ0, λ1, '', sep='\\n')\n",
    "\n",
    "# Apply \"M\" to the first (0) and the second (1) eigenvectors and store them\n",
    "Me0 = M @ e0\n",
    "Me1 = M @ e1\n",
    "print('M applied to the eigenvectors:', Me0, Me1, '', sep='\\n')\n",
    "\n",
    "# Multiply each eigenvector by its eigenvalue\n",
    "λ0e0 = λ0 * e0\n",
    "λ1e1 = λ1 * e1\n",
    "print('Eigenvalues multiplied by the eigenvectors:', λ0e0, λ1e1, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how multiplying the eigenvectors by their corresponding eigenvalues, $\\lambda_{i} \\vec{e}_i$, returns the vectors resulting when we applying the linear transformation to such eigenvector, $\\textbf{M} \\vec{e}_i$, because, by construction:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{M} \\vec{e}_i = \\lambda_{i} \\vec{e}_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input \"e0\" (red) and output \"Me0\" (green) vectors\n",
    "gg = (\n",
    "    p9.ggplot(data=df)\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=e0[0], yend=e0[1],\n",
    "        color='red',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=Me0[0], yend=Me0[1],\n",
    "        color='green',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    "    + p9.xlim(-1, 5) + p9.ylim(-1, 5) + p9.xlab('x') + p9.ylab('y')\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "    \n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input \"e1\" (red) and output \"Me1\" (green) vectors\n",
    "gg = (\n",
    "    \n",
    "    # Create the scatter plot with Plotnine\n",
    "    p9.ggplot(data=df)\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=e1[0], yend=e1[1],\n",
    "        color='red',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=Me1[0], yend=Me1[1],\n",
    "        color='green',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    "    + p9.xlim(-1, 1) + p9.ylim(-1, 1) + p9.xlab('x') + p9.ylab('y')\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, after passing through $\\textbf{M}$, the first eigenvector $\\vec{e}_0$ is *reversed* and *elongated* ($\\lambda_0 = -4.236$) whilst the second eigenvector $\\vec{e}_1$ is *shortened* ($\\lambda_1 = +0.236$), but both of them keep their directions intact. Note how their corresponding eigenvalues encode this outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<b>Practice 2:</b> Get the **eigenvalues** and the **eigenvectors** of the **covariance matrix** that we have already obtained for the **height** and **weight** after z-scoring the data.\n",
    "\n",
    "1) In the 1<sup>st</sup> code cell below, use the <b>NumPy</b> function `np.linalg.eig()` to compute the the eigenvalues and the eigenvectors of the covariance matrix between the variables `Z-Height` and `Z-Weight` from our `df_DropNan` dataset (`Sigma_Z`).\n",
    "2) In the 2<sup>nd</sup> code cell below, breakdown the `EigResult` object returned by `np.linalg.eig()` and store both eigenvectors as `e0` and `e1` and both eigenvalues as `λ0` and `λ1`.\n",
    "3) In the 3<sup>rd</sup> code cell below, represent `e0` and `e1` on the scatter plot with `Z-Weight` versus `Z-Height`. **What do you observe?**\n",
    "4) In the 4<sup>th</sup> code cell below, represent again `e0` and `e1` on the scatter plot, but scaling each eigenvector by its corresponding eigenvalue. **What do you observe now?**\n",
    "\n",
    "<b>Note:</b> Uncomment and fill only those code lines with underscores `___`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1st code cell)\n",
    "\n",
    "# Show the the z-scored covariance matrix\n",
    "# print('Covariance matrix:', ___, '', sep='\\n')\n",
    "\n",
    "# Get the eigenvectors and eigenvalues of the z-scored covariance matrix\n",
    "# eigen = ___(___)\n",
    "\n",
    "# Print the eigenvectors and eigenvalues of the z-scored covariance matrix\n",
    "# print('EigResult object:', eigen, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (1st code cell)\n",
    "\n",
    "# Show the the z-scored covariance matrix\n",
    "print('Covariance matrix:', Sigma_Z, '', sep='\\n')\n",
    "\n",
    "# Get the eigenvectors and eigenvalues of the z-scored covariance matrix\n",
    "eigen = np.linalg.eig(Sigma_Z)\n",
    "\n",
    "# Print the eigenvectors and eigenvalues of the z-scored covariance matrix\n",
    "print('EigResult object:', eigen, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2nd code cell)\n",
    "\n",
    "# Store the first (0) and the second (1) eigenvectors and show them\n",
    "# e0 = eigen.___[___, ___]\n",
    "# e1 = eigen.___[___, ___]\n",
    "# print('Eigenvectors:', ___, ___, '', sep='\\n')\n",
    "\n",
    "# Store the first (0) and the second (1) eigenvalues and show them\n",
    "# λ0 = eigen.___[___]\n",
    "# λ1 = eigen.___[___]\n",
    "# print('Eigenvalues:', ___, ___, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (2nd code cell)\n",
    "\n",
    "# Store the first (0) and the second (1) eigenvectors and show them\n",
    "e0 = eigen.eigenvectors[:, 0]\n",
    "e1 = eigen.eigenvectors[:, 1]\n",
    "print('Eigenvectors:', e0, e1, '', sep='\\n')\n",
    "\n",
    "# Store the first (0) and the second (1) eigenvalues and show them\n",
    "λ0 = eigen.eigenvalues[0]\n",
    "λ1 = eigen.eigenvalues[1]\n",
    "print('Eigenvalues:', λ0, λ1, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3rd code cell)\n",
    "\n",
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Z-Height', y='Z-Weight'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "\n",
    "     # Plot the first eigenvector\n",
    "#    + p9.geom_segment(\n",
    "#        x=0, y=0,\n",
    "#        xend=___, yend=___],\n",
    "#        color='red',\n",
    "#        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "#     )\n",
    "\n",
    "     # Plot the second eigenvector\n",
    "#    + p9.geom_segment(\n",
    "#        x=0, y=0,\n",
    "#        xend=___, yend=___,\n",
    "#        color='green',\n",
    "#        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "#     )\n",
    "\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (3rd code cell)\n",
    "\n",
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Z-Height', y='Z-Weight'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "\n",
    "    # Plot the first eigenvector\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=e0[0], yend=e0[1],\n",
    "        color='red',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "\n",
    "    # Plot the second eigenvector\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=e1[0], yend=e1[1],\n",
    "        color='green',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4th code cell)\n",
    "\n",
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Z-Height', y='Z-Weight'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "\n",
    "    # Plot the second eigenvector (scaled)\n",
    "#    + p9.geom_segment(\n",
    "#        x=0, y=0,\n",
    "#        xend=___, yend=___,\n",
    "#        color='red',\n",
    "#        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "#     )\n",
    "#\n",
    "#    # Plot the second eigenvector (scaled)\n",
    "#    + p9.geom_segment(\n",
    "#        x=0, y=0,\n",
    "#        xend=___, yend=___,\n",
    "#        color='green',\n",
    "#        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "#     )\n",
    "\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# (4th code cell)\n",
    "\n",
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan, mapping=p9.aes(x='Z-Height', y='Z-Weight'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=λ0*e0[0], yend=λ0*e0[1],\n",
    "        color='red',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_segment(\n",
    "        x=0, y=0,\n",
    "        xend=λ1*e1[0], yend=λ1*e1[1],\n",
    "        color='green',\n",
    "        arrow=p9.arrow(angle=30, length=0.05, ends=\"last\", type=\"closed\")\n",
    "     )\n",
    "    + p9.geom_vline(xintercept=0, color='darkgrey')\n",
    "    + p9.geom_hline(yintercept=0, color='darkgrey')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>\n",
    "Practice 2 ends here.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The meaning of the covariance matrix\n",
    "\n",
    "So far, given two z-scaled columns of our \"Brain Size\" dataset (`'Z-Height'` and `'Z-Weight'`), we have been able to compute the covariance matrix and to find its corresponding eigenvectors and eigenvalues . At the end of **Practice 2**, we intuitively verified a crucial insight about the covariance matrix: the **eigenvector with the largest eigenvalue** (in absolute value) is denoting the **direction along which our data exhibits the largest variability** contribution, and so on, the eigenvector with the second largest eigenvalue (in absolute value) is denoting the direction along which our data exhibits the second largest variability contribution. This is the key principle behind the [**Principal Component Analysis**](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA), a [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) technique of extreme utility in almost any data analysis scenario (if used correctly avoiding the typical error of over-interpretation). \n",
    " \n",
    "Having said that, now, let's organize the eigenvalues and eigenvectors of the z-scored covariance matrix `Sigma_Z` as nice list of `(eigenvalues, eigenvector)` tuples sorted in decreasing order according the absolute values of their corresponding eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nice list of (|eigenvalue|, eigenvector) tuples\n",
    "eigentuples = [(np.abs(eigen.eigenvalues[i]), eigen.eigenvectors[:, i]) for i in range(len(eigen.eigenvalues))]\n",
    "\n",
    "# Show the list of \"eigentuples\"\n",
    "eigentuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the \"eigentuples\" from the highest to the lowest eigenvalue absolute value\n",
    "eigentuples = sorted(eigentuples, reverse=True)\n",
    "\n",
    "# Show the list of sorted \"eigentuples\"\n",
    "eigentuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b>\n",
    "In our particular case, the tuples are already sorted, but we decided to \"sort\" them anyway so we don't loose any generality in our workflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "Ok, but what do we mean by *dimensionality reduction*? Our z-scaled data has two magnitudes (*features*), `Z-Height` and `Z-Weight` that have been tracked for multiple individuals (*samples*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show our dataset\n",
    "df_DropNan[['Z-Height', 'Z-Weight']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples and features in our dataset\n",
    "n_samples, n_features = df_DropNan[['Z-Height', 'Z-Weight']].shape\n",
    "\n",
    "# Print the number of samples and features in our dataset\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, our dataset has 38 points (samples) that lay on a bi-dimensional space (one dimension for each feature), so we can say that the dimensionality of our dataset is $(38 \\times 2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b>\n",
    "In a more general and typical case, the number of features is orders of magnitude higher than the number of samples. For example, a typical proteomics dataset has $\\sim 10^{3}$ quantified proteins (features) across $\\sim 10^{1}$ patients (samples), with a dataset dimensionality of $(\\sim 10^{1} \\times \\sim 10^{3})$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collapse the dimensions of our dataset into a lower-dimensionality space, the [basis](https://www.google.com/search?client=firefox-b-d&q=base+of+a+linear+space) of which comprises the eigenvectors of the covariance matrix. In order to do that, we first need to prepare the projection matrix:\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{pmatrix} \\vec{e}_1 & \\vec{e}_2 \\end{pmatrix} =\n",
    " \\begin{pmatrix} \\begin{pmatrix} e_{11} \\\\ e_{21} \\end{pmatrix} & \\begin{pmatrix} e_{12} \\\\ e_{22} \\end{pmatrix} \\end{pmatrix} =\n",
    " \\begin{pmatrix} \\begin{pmatrix} 0.7071 \\\\ 0.7071 \\end{pmatrix} & \\begin{pmatrix} -0.7071 \\\\ 0.7071 \\end{pmatrix} \\end{pmatrix} =\n",
    " \\begin{pmatrix} 0.7071 & -0.7071 \\\\ 0.7071 & 0.7071 \\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the projection matrix with all eigenvectors\n",
    "W = np.transpose(np.array([eigenvector for (eigenvalue, eigenvector) in eigentuples]))\n",
    "\n",
    "# Show the shape of the projection matrix\n",
    "print('Projection matrix shape:', W.shape, '', sep='\\n')\n",
    "\n",
    "# Show the projection matrix\n",
    "print('Projection matrix:', W, '', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Caveat:</b> Remember, eigenvector <b><u>as columns</u></b> of the projection matrix, (not as rows!):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before performing the dimensionality reduction, usually we must decide how many dimensions should we keep, this means, how many eigenvectors of the projection matrix should we keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of top-k eigenvectors of the projection matrix to keep\n",
    "k = 2\n",
    "\n",
    "# Select top-k eigenvectors of the projection matrix\n",
    "W_Projection = W[0:k]\n",
    "\n",
    "# Perform the dimensionality reduction: (38 x 2) @ (2 x 2) = (38 x 2)\n",
    "df_DropNan_Projection = df_DropNan[['Z-Height', 'Z-Weight']] @ W_Projection\n",
    "\n",
    "# Show the shape of the projection matrix\n",
    "print('Projection matrix shape:', df_DropNan_Projection.shape, '', sep='\\n')\n",
    "# We reduced from (38 x 2) to (38 x 2) (The same dimensionality! No reduction!)\n",
    "\n",
    "# Show the projection matrix\n",
    "df_DropNan_Projection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b>\n",
    "In a more general and typical case, this step would have trully reduced the dataset dimensionality instead of leaving it unchanged. For example, a dataset with dimensionality $(9 \\times 9000)$ would have been reduced to a dataset of dimensionality $(9 \\times 2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "df_DropNan_Projection.columns = [f'PC{str(colname+1)}' for colname in list(df_DropNan_Projection)]\n",
    "\n",
    "# TODO\n",
    "df_DropNan_Projection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot with Plotnine (z-score data)\n",
    "gg = (\n",
    "    p9.ggplot(data=df_DropNan_Projection, mapping=p9.aes(x='PC1', y='PC2'))\n",
    "    + p9.geom_point()\n",
    "    + p9.theme(figure_size=(4, 4))\n",
    "\n",
    "    + p9.geom_vline(xintercept=0, color='green')\n",
    "    + p9.geom_hline(yintercept=0, color='red')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_total = sum(eig_vals_sorted)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,n_features+1), cum_explained_variance, '-o')\n",
    "plt.xticks(np.arange(1,n_features+1))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj = df_DropNan[['Z-Height', 'Z-Weight']].dot(W.T)\n",
    "\n",
    "print(X_proj.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Exercise_Prostate_Cancer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
